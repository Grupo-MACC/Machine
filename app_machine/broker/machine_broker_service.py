# -*- coding: utf-8 -*-
"""RabbitMQ broker para Machine.

Contrato (manteniendo funcionalidad original):
    - Warehouse publica 1 pieza por mensaje:
        * machine.a (piezas tipo A)
        * machine.b (piezas tipo B)
      Machine consume SOLO el tipo configurado por entorno.
    - Reparto justo entre r√©plicas:
        * QoS prefetch_count=1
        * ACK SOLO cuando termina la fabricaci√≥n (salida del context manager)
    - Al terminar fabricaci√≥n publica:
        * piece.done (payload devuelto por Machine.fabricate_piece)
    - SAGA cancelaci√≥n fabricaci√≥n:
        * consume cmd.machine.cancel
        * a√±ade order_id a blacklist (DB compartida)
        * publica evt.machine.canceled como confirmaci√≥n a Warehouse
    - Auth events:
        * consume auth.running / auth.not_running
        * si running, descarga public-key y la guarda en PUBLIC_KEY_PATH
    - Logger:
        * publica logs estructurados a exchange_logs
"""

from __future__ import annotations

import asyncio
import json
import logging
import os

import httpx
from aio_pika import Message

from consul_client import get_consul_client
from dependencies import get_machine
from microservice_chassis_grupo2.core.rabbitmq_core import (
    PUBLIC_KEY_PATH,
    declare_exchange,
    declare_exchange_logs,
    get_channel,
)

logger = logging.getLogger(__name__)

# =============================================================================
# Constantes RabbitMQ (routing keys / colas / topics)
# =============================================================================

# --- Config por entorno (mismo c√≥digo para machine-A / machine-B) ---
MACHINE_PIECE_TYPE = os.getenv("MACHINE_PIECE_TYPE", "A")  # "A" o "B"
SERVICE_ID = os.getenv("SERVICE_ID", "machine-1")

# Routing key que esta instancia consume (machine.a o machine.b)
RK_CMD_DO_PIECE = f"todo.machine.{MACHINE_PIECE_TYPE.lower()}"

# Cola compartida entre r√©plicas del mismo tipo (competici√≥n)
QUEUE_DO_PIECE = f"machine_{MACHINE_PIECE_TYPE.lower()}_queue"

# --- Eventos y comandos de la SAGA cancelaci√≥n (Warehouse -> Machine -> Warehouse) ---
RK_CMD_MACHINE_CANCEL = "cmd.machine.cancel"
RK_EVT_MACHINE_CANCELED = "evt.machine.canceled"

# Cola compartida entre r√©plicas: solo 1 procesa cada cmd.machine.cancel
QUEUE_MACHINE_CANCEL = "machine_cancel_queue"

# --- Evento de pieza terminada (Machine -> Warehouse) ---
RK_EVT_PIECE_DONE = "piece.done"

# --- Auth events (exchange general) ---
RK_AUTH_RUNNING = "auth.running"
RK_AUTH_NOT_RUNNING = "auth.not_running"
QUEUE_AUTH_EVENTS = "machine_queue"  # se mantiene por compatibilidad (nombre hist√≥rico)

# --- Logger topics ---
TOPIC_INFO = "machine.info"
TOPIC_ERROR = "machine.error"
TOPIC_DEBUG = "machine.debug"


# =============================================================================
# 1) HELPERS internos (publicaci√≥n consistente)
# =============================================================================
#region 0. HELPERS
def _build_json_message(payload: dict) -> Message:
    """Construye un mensaje JSON persistente."""
    return Message(
        body=json.dumps(payload).encode(),
        content_type="application/json",
        delivery_mode=2,  # persistente
    )


async def _publish_exchange(routing_key: str, payload: dict) -> None:
    """Publica un payload JSON al exchange general (declare_exchange).

    Nota:
        Mantengo el mismo exchange que tu versi√≥n original para no cambiar
        sem√°ntica/infra de routing.
    """
    connection, channel = await get_channel()
    try:
        exchange = await declare_exchange(channel)
        await exchange.publish(message=_build_json_message(payload), routing_key=routing_key)
    finally:
        await connection.close()


def _require_fields(data: dict, required: tuple[str, ...], context: str) -> bool:
    """Valida que existan campos obligatorios en el payload."""
    missing = [k for k in required if data.get(k) is None]
    if not missing:
        return True
    logger.error("[MACHINE] ‚ùå Payload inv√°lido en %s (faltan %s): %s", context, missing, data)
    return False

def _internal_ca_file() -> str:
    """
    Devuelve la ruta del CA bundle para llamadas internas HTTPS.

    Por qu√©:
        - Los microservicios est√°n usando certificados firmados por una CA privada.
        - httpx por defecto valida contra el bundle del sistema/certifi.
        - Si no le pasas tu CA, obtendr√°s CERTIFICATE_VERIFY_FAILED.

    Prioridad:
        1) INTERNAL_CA_FILE
        2) CONSUL_CA_FILE
        3) /certs/ca.pem (convenci√≥n del proyecto)
    """
    return os.getenv("INTERNAL_CA_FILE") or os.getenv("CONSUL_CA_FILE") or "/certs/ca.pem"

async def _download_auth_public_key(auth_base_url: str) -> str:
    """
    Descarga la clave p√∫blica de Auth usando HTTPS con verificaci√≥n por CA privada.

    Args:
        auth_base_url: Base URL (p.ej. "https://auth:5004")

    Returns:
        El texto PEM de la clave p√∫blica.

    Nota:
        - Separar esta funci√≥n facilita reintentos.
    """
    async with httpx.AsyncClient(verify=_internal_ca_file(), timeout=5.0) as client:
        resp = await client.get(f"{auth_base_url}/auth/public-key")
        resp.raise_for_status()
        return resp.text


async def _ensure_auth_public_key(max_attempts: int = 20, base_delay: float = 0.25) -> None:
    """
    Asegura que existe la clave p√∫blica de Auth en PUBLIC_KEY_PATH.

    Estrategia simple:
        - Intenta resolver Auth por Consul (passing=true).
        - Si a√∫n no hay instancias passing (race al arrancar), reintenta con backoff.
        - Cuando lo resuelve, descarga la clave con TLS verify (CA privada) y la guarda.

    Por qu√©:
        - auth.running se publica antes de que Auth est√© realmente "ready" (FastAPI a√∫n no sirve HTTP).
        - Por tanto, al recibir el evento, Consul puede devolver 0 passing temporalmente.
    """
    for attempt in range(1, max_attempts + 1):
        try:
            auth_base_url = await get_consul_client().get_service_base_url("auth")
            public_key = await _download_auth_public_key(auth_base_url)

            # Escritura directa (simple). Si quieres m√°s robustez: escribir a .tmp y renombrar.
            with open(PUBLIC_KEY_PATH, "w", encoding="utf-8") as f:
                f.write(public_key)

            logger.info("[MACHINE] ‚úÖ Clave p√∫blica de Auth guardada en %s", PUBLIC_KEY_PATH)
            return

        except Exception as exc:
            # OJO: esto NO es un error grave. Es normal durante el arranque.
            logger.warning(
                "[MACHINE] ‚è≥ Auth a√∫n no est√° 'passing' o no responde. Reintento %s/%s. Motivo: %s",
                attempt, max_attempts, exc
            )

            # Backoff suave (capado)
            delay = min(2.0, base_delay * (2 ** (attempt - 1)))
            await asyncio.sleep(delay)

    raise RuntimeError("No se pudo obtener la clave p√∫blica de Auth tras varios reintentos.")


# =============================================================================
# 2) HANDLER: DO PIECES (Warehouse -> Machine)
# =============================================================================
#region 1. FABRICATION
async def handle_do_pieces(message) -> None:
    """Consume 1 pieza por mensaje y publica `piece.done` cuando se fabrica.

    Contrato esperado (Warehouse):
        - piece_id: str UUID
        - order_id: int
        - piece_type: 'A'|'B'
        - order_date: str ISO (opcional; hoy no se usa pero se acepta)

    Propiedades importantes:
        - ACK al final del bloque `async with message.process()`:
          si fabricas lento, no se ACKea hasta terminar.
        - prefetch_count=1: reparto justo entre r√©plicas.
    """
    async with message.process():
        data = json.loads(message.body)

        if not _require_fields(data, ("piece_id", "order_id", "piece_type"), context="do_piece"):
            return

        piece_id = data.get("piece_id")
        order_id = int(data.get("order_id"))
        piece_type = data.get("piece_type")
        order_date = data.get("order_date")

        if piece_type not in ("A", "B"):
            logger.error("[MACHINE] ‚ùå piece_type inv√°lido: %s (data=%s)", piece_type, data)
            return

        machine = await get_machine()

        # 1) Idempotencia: si ya est√° procesada, ACK y fuera (no republish)
        if await machine.is_piece_already_processed(piece_id):
            logger.info("[MACHINE] ‚ôªÔ∏è Duplicado piece_id=%s ‚Üí ACK sin publicar", piece_id)
            return

        # 2) Blacklist: si la order est√° cancelada, registrar/consumir sin publicar
        if await machine.is_order_blacklisted(order_id):
            logger.info("[MACHINE] üö´ Order %s cancelada ‚Üí skip piece %s", order_id, piece_id)
            # Mantengo tu comportamiento: llamas fabricate_piece (posible registro SKIPPED)
            await machine.fabricate_piece(order_id, piece_id, piece_type, order_date)
            return

        # 3) Fabricar (persistir√° inflight + fabricated)
        done_event = await machine.fabricate_piece(order_id, piece_id, piece_type, order_date)

        # Si decide no fabricar (blacklist/duplicado), no publicamos
        if not done_event or done_event.get("result") != "MANUFACTURED":
            return

        # 4) Publicar evento de pieza fabricada
        await _publish_exchange(routing_key=RK_EVT_PIECE_DONE, payload=done_event)

        # 5) Marcar como publicado en DB
        await machine.mark_done_published(piece_id)


async def consume_do_pieces_events() -> None:
    """Arranca el consumer del tipo configurado (A/B).

    Claves para reparto justo:
        - prefetch_count=1
        - ACK al final del handler
    """
    connection, channel = await get_channel()
    try:
        await channel.set_qos(prefetch_count=1)  # CR√çTICO

        exchange = await declare_exchange(channel)

        queue = await channel.declare_queue(QUEUE_DO_PIECE, durable=True)
        await queue.bind(exchange, routing_key=RK_CMD_DO_PIECE)
        await queue.consume(handle_do_pieces)

        logger.info(
            "[MACHINE-%s] üü¢ Escuchando '%s' en cola '%s' ‚Ä¶",
            MACHINE_PIECE_TYPE,
            RK_CMD_DO_PIECE,
            QUEUE_DO_PIECE,
        )

        await publish_to_logger(
            message={"message": "Escuchando piezas", "routing_key": RK_CMD_DO_PIECE, "queue": QUEUE_DO_PIECE},
            topic=TOPIC_INFO,
        )

        await asyncio.Future()
    finally:
        # Normalmente no se llega (Future infinito)
        await connection.close()


# =============================================================================
# 3) HANDLER: CANCEL (Warehouse -> Machine -> Warehouse)
# =============================================================================
#region 2. CANCEL SAGA
async def handle_cmd_machine_cancel(message) -> None:
    """Procesa cmd.machine.cancel.

    Payload esperado:
        {"order_id": int, "saga_id": str (opcional)}

    Efecto:
        - Inserta order_id en blacklist (DB compartida).
        - Publica evt.machine.canceled como confirmaci√≥n hacia Warehouse.
    """
    async with message.process():
        data = json.loads(message.body)

        if not _require_fields(data, ("order_id",), context=RK_CMD_MACHINE_CANCEL):
            return

        order_id = int(data.get("order_id"))
        saga_id = data.get("saga_id")

        machine = await get_machine()
        await machine.add_to_blacklist(order_id, reason="CANCEL_MANUFACTURING")

        logger.warning("[MACHINE] üõë Cancel registrada en blacklist: order_id=%s", order_id)

        payload = {
            "order_id": order_id, 
            "machine_type": MACHINE_PIECE_TYPE
            }
        
        if saga_id is not None:
            payload["saga_id"] = str(saga_id)

        logger.info("[MACHINE] üì£ Publicando evt.machine.canceled: %s", payload)
        await _publish_exchange(routing_key=RK_EVT_MACHINE_CANCELED, payload=payload)


async def consume_cmd_machine_cancel() -> None:
    """Escucha cmd.machine.cancel en una cola compartida.

    Sem√°ntica:
        - R√©plicas compiten por la cola -> solo una procesa el comando.
        - Como la blacklist est√° en BD compartida, el efecto es global.
    """
    connection, channel = await get_channel()
    try:
        exchange = await declare_exchange(channel)

        queue = await channel.declare_queue(QUEUE_MACHINE_CANCEL, durable=True)
        await queue.bind(exchange, routing_key=RK_CMD_MACHINE_CANCEL)
        await queue.consume(handle_cmd_machine_cancel)

        logger.info(
            "[MACHINE] üü† Escuchando '%s' en cola '%s' (competing consumers)",
            RK_CMD_MACHINE_CANCEL,
            QUEUE_MACHINE_CANCEL,
        )

        await publish_to_logger(
            message={"message": "Escuchando cmd.machine.cancel", "routing_key": RK_CMD_MACHINE_CANCEL, "queue": QUEUE_MACHINE_CANCEL},
            topic=TOPIC_INFO,
        )

        await asyncio.Future()
    finally:
        await connection.close()


# =============================================================================
# 4) AUTH EVENTS (Auth -> Machine)
# =============================================================================
#region 3. AUTH EVENTS
async def handle_auth_events(message) -> None:
    """
    Gestiona eventos de auth.running / auth.not_running.

    Nota importante:
        - Aunque recibamos 'running', Auth puede no estar listo a√∫n (FastAPI a√∫n no sirve HTTP).
        - Por eso hacemos reintentos contra Consul (passing=true) y luego descargamos la clave.
    """
    async with message.process():
        data = json.loads(message.body)
        if data.get("status") != "running":
            return

        try:
            await _ensure_auth_public_key()
            await publish_to_logger(
                message={"message": "Clave p√∫blica guardada", "path": PUBLIC_KEY_PATH},
                topic=TOPIC_INFO,
            )
        except Exception as exc:
            logger.error("[MACHINE] ‚ùå Error obteniendo clave p√∫blica: %s", exc)
            await publish_to_logger(
                message={"message": "Error clave p√∫blica", "error": str(exc)},
                topic=TOPIC_ERROR,
            )


async def consume_auth_events() -> None:
    """Consumer de auth.running/auth.not_running.

    Importante:
        En tu versi√≥n original este consumer NO bloqueaba (faltaba Future),
        lo que puede dejarlo inactivo si se ejecuta como task.
    """
    connection, channel = await get_channel()
    try:
        exchange = await declare_exchange(channel)

        queue = await channel.declare_queue(QUEUE_AUTH_EVENTS, durable=True)
        await queue.bind(exchange, routing_key=RK_AUTH_RUNNING)
        await queue.bind(exchange, routing_key=RK_AUTH_NOT_RUNNING)
        await queue.consume(handle_auth_events)

        logger.info("[MACHINE] üü¢ Escuchando eventos de auth (running/not_running) en %s", QUEUE_AUTH_EVENTS)
        await publish_to_logger(
            message={"message": "Escuchando eventos de auth", "queue": QUEUE_AUTH_EVENTS},
            topic=TOPIC_INFO,
        )

        await asyncio.Future()
    finally:
        await connection.close()


# =============================================================================
# 5) LOGGER
# =============================================================================
#region 4. LOGGER
async def publish_to_logger(message: dict, topic: str) -> None:
    """Env√≠a un log estructurado al sistema de logs.

    Args:
        message: dict con campos extra.
        topic: 'machine.info' | 'machine.error' | 'machine.debug' | ...
    """
    connection = None
    try:
        connection, channel = await get_channel()
        exchange = await declare_exchange_logs(channel)

        service, severity = (topic.split(".", 1) + ["info"])[:2]

        log_data = {
            "measurement": "logs",
            "service": service,
            "severity": severity,
            **message,
        }

        await exchange.publish(message=_build_json_message(log_data), routing_key=topic)

    except Exception:
        logger.exception("[MACHINE] Error publicando al logger")
    finally:
        if connection:
            await connection.close()
